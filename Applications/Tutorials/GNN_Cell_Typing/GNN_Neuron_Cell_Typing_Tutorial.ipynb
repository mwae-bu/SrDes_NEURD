{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f3b0ed2-e4fd-44bd-bf55-17bf2137cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c007dbc5-ff4b-4ef3-bfe6-c237df41b988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nShow the general pipeline used for the GNN models used for cell typing\\nin the neuron and limb based models referenced in the NEURD paper. This \\ntutorial shows how to use the models for inference runs. \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Show the general pipeline used for the GNN models used for cell typing\n",
    "in the neuron and limb based models referenced in the NEURD paper. This \n",
    "tutorial shows how to use the models for inference runs. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdf4879-bcf0-47a1-8ba5-d6ae92fce1f0",
   "metadata": {},
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ce0187-7c64-4aa7-ad1d-d7a08a0afb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch-scatter -f https://data.pyg.org/whl/torch-1.10.2+cpu.html\n",
    "# !pip3 install torch-sparse -f https://data.pyg.org/whl/torch-1.10.2+cpu.html\n",
    "# !pip3 install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b27b6e-5e80-4594-9a73-e9963a4e0392",
   "metadata": {},
   "source": [
    "# Model Training Background Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f79bba24-e2fe-42b1-a2b0-e7af9a6b27f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Standard training \n",
    "of GNN models using pytorch geometric was used with the following hyper parameters\n",
    "\n",
    "Neuron Model\n",
    "------------\n",
    "1) lr = 0.0001\n",
    "2) label_weights = {\n",
    "     '23P': 0.8,\n",
    "     '4P': 0.5,\n",
    "     '5P-IT': 1,\n",
    "     '5P-NP': 1,\n",
    "     '5P-PT': 1,\n",
    "     '6P-CT': 0.8,\n",
    "     '6P-IT': 1,\n",
    "     'BC': 1,\n",
    "     'BPC': 1,\n",
    "     'MC': 1,\n",
    "     'NGC': 1\n",
    "    }\n",
    "3) Dropout (0.5)\n",
    "4) torch.optim.Adam optimizer\n",
    "5) batch_size = 64\n",
    "6) 60/20/20 train/validation/test split\n",
    "\n",
    "Limb Model\n",
    "----------\n",
    "1) lr = 0.001 ** only learning hyper parameter that is different **\n",
    "2) label_weights = {\n",
    "     '23P': 0.8,\n",
    "     '4P': 0.5,\n",
    "     '5P-IT': 1,\n",
    "     '5P-NP': 1,\n",
    "     '5P-PT': 1,\n",
    "     '6P-CT': 0.8,\n",
    "     '6P-IT': 1,\n",
    "     'BC': 1,\n",
    "     'BPC': 1,\n",
    "     'MC': 1,\n",
    "     'NGC': 1\n",
    "    }\n",
    "3) Dropout (0.5)\n",
    "4) torch.optim.Adam optimizer\n",
    "5) batch_size = 64\n",
    "6) 60/20/20 train/validation/test split\n",
    "\n",
    "Note: training \n",
    "\"\"\"\n",
    "\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4aa093-1d8a-419b-a4b2-bb167e54e3ef",
   "metadata": {},
   "source": [
    "# Step 0: Accessing ground truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "041f512b-b6b5-4e54-bf1c-d33380a27442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCan get ground truth cell typing labels from the caveclient\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Can get ground truth cell typing labels from the caveclient\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d71432f1-7285-4188-8381-50857e1b7e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/python_jsonschema_objects/__init__.py:113: UserWarning: Schema id not specified. Defaulting to 'self'\n",
      "  warnings.warn(\"Schema id not specified. Defaulting to 'self'\")\n"
     ]
    }
   ],
   "source": [
    "from neurd.vdi_microns_cave import volume_data_interface as vdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dca7815e-e0d2-4213-9ccd-bc80019c9ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe minnie public api wasn't working at the time of development so we\\npulled down the public cell typing table ahead of time and now\\njust need to load it locally\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The minnie public api wasn't working at the time of development so we\n",
    "pulled down the public cell typing table ahead of time and now\n",
    "just need to load it locally\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc8b17f7-2516-4765-99ad-2e6cd7278b29",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './training_data/public_cave_ground_truth_cell_types_with_nucleus.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasci_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pandas_utils \u001b[38;5;28;01mas\u001b[39;00m pu\n\u001b[0;32m----> 2\u001b[0m df_labels \u001b[38;5;241m=\u001b[39m \u001b[43mpu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./training_data/public_cave_ground_truth_cell_types_with_nucleus.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df_labels\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasci_tools/pandas_utils.py:777\u001b[0m, in \u001b[0;36mcsv_to_df\u001b[0;34m(csv_filepath, remove_unnamed_columns)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcsv_to_df\u001b[39m(csv_filepath,remove_unnamed_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 777\u001b[0m     return_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_filepath\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m remove_unnamed_columns:\n\u001b[1;32m    779\u001b[0m         return_df \u001b[38;5;241m=\u001b[39m pu\u001b[38;5;241m.\u001b[39mremove_unnamed_columns(return_df)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './training_data/public_cave_ground_truth_cell_types_with_nucleus.csv'"
     ]
    }
   ],
   "source": [
    "from datasci_tools import pandas_utils as pu\n",
    "df_labels = pu.csv_to_df(\"./training_data/public_cave_ground_truth_cell_types_with_nucleus.csv\")\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ffe592-1635-47f7-ba58-a688d99f551e",
   "metadata": {},
   "source": [
    "## how to pull down the mesh for a segment id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b1e46-1de1-4334-8acc-636f09b6d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_id = 864691135694415551"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c6ee37-bb81-444d-83cf-b03bf36a793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = vdi.fetch_segment_id_mesh(segment_id)\n",
    "mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5d083-a16c-4e8c-bd91-8405cbdfe5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_labels.query(f\"segment_id == {segment_id}\")['cell_type'].to_list()[0]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a835c8-8286-4ffb-83ad-b65b27bc38a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasci_tools import ipyvolume_utils as ipvu\n",
    "\n",
    "ipvu.plot_objects(mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2bb8d8-61b4-4718-aeec-c54bcb2fc450",
   "metadata": {},
   "source": [
    "## Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cffcd1b-8645-4f7b-8a4a-eb107cf4e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Would then just decompose using the given tutorials to get the \n",
    "neuron objects to be used in the GNN classification\n",
    "\n",
    "Example: NEURD/Applications/Tutorials/Auto_Proof_Pipeline/Single_Soma_Exc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136efc3d-8223-4fb5-ba75-8af71f40087b",
   "metadata": {},
   "source": [
    "## ** If want output to completely match those the pretrained models were developed with then need to compute spine volume with convex hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239bc3f5-1d5b-4528-b789-2bec4c31db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mesh_tools import trimesh_utils as tu\n",
    "\n",
    "tu.mesh_volume_old = tu.mesh_volume\n",
    "def new_mesh_volume_func(*args,**kwargs):\n",
    "    kwargs[\"watertight_method\"] = \"convex_hull\"\n",
    "    return tu.mesh_volume_old(\n",
    "        *args,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "tu.mesh_volume = new_mesh_volume_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc1d343-bf61-4ea1-9fd5-19b5ed0193b1",
   "metadata": {},
   "source": [
    "# Step 0: Pulling Data Previously Computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bda265-8adb-428f-b74c-1bcebff8f4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurd import neuron_utils as nru\n",
    "from neuron_morphology_tools import neuron_nx_utils as nxu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bc5c42-16fe-499b-9209-8a291faef5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "exc_neuron_obj = \"../Auto_Proof_Pipeline/Single_Soma_Exc/864691134917511946_auto_proof.pbz2\"\n",
    "exc_neuron_mesh = \"../Auto_Proof_Pipeline/Single_Soma_Exc/864691134917511946.off\"\n",
    "exc_cell_type = \"5P-IT\"\n",
    "\n",
    "neuron_obj_exc = nru.decompress_neuron(\n",
    "    exc_neuron_obj,\n",
    "    original_mesh=exc_neuron_mesh\n",
    ")\n",
    "neuron_obj_exc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4bc207-4363-443b-8f1e-008ed5ca50d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inh_neuron_obj = \"../Auto_Proof_Pipeline/Single_Soma_Inh/864691135567721964_auto_proof.pbz2\"\n",
    "inh_neuron_mesh = \"../Auto_Proof_Pipeline/Single_Soma_Inh/864691135567721964.off\"\n",
    "inh_cell_type = \"MC\"\n",
    "\n",
    "neuron_obj_inh = nru.decompress_neuron(\n",
    "    inh_neuron_obj,\n",
    "    original_mesh=inh_neuron_mesh\n",
    ")\n",
    "neuron_obj_inh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a64f0-7749-497f-9db6-8416989fa8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_exc = neuron_obj_exc.neuron_graph_after_proof\n",
    "G_exc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21721553-b931-43ef-8e72-bc2a675de713",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_inh = neuron_obj_inh.neuron_graph_after_proof\n",
    "G_inh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe725e2a-a4c6-4273-8589-fbc55ec880ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "nxu.draw_tree(G_exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4153d0d-e837-482f-a961-e7c329ed94f5",
   "metadata": {},
   "source": [
    "# Step 1: Transforming networkx graphs into dataset objects (that can be easily converted to pytorch geometric dataset objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec400716-8de5-4411-b93c-b9099704f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurd.gnn_cell_typing_utils import NeuronGraphData,OutputClassConfig\n",
    "from neurd import gnn_cell_typing_utils as gnnu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee13dd-943c-4fec-bf46-cac89de3c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gs = [\n",
    "    (G_exc,exc_cell_type),\n",
    "    (G_inh,inh_cell_type),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f913e8-8397-48e0-95fe-c39e4617a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting networkx graphs into dataset objects\n",
    "G_data = [NeuronGraphData(k,label=v) for k,v in Gs]\n",
    "Gd = G_data[0]\n",
    "Gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62583a1-a35f-4541-b73f-d6d3544897c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deciding whether to use full neurons or limbs of neurons as graph data\n",
    "\n",
    "# full neurons as instances\n",
    "graph_data_list_neuron = G_data\n",
    "\n",
    "#neuron limbs as instances\n",
    "graph_data_list_limbs = [limb_G for neurons in G_data for limb_G in neurons.limb_data_objs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bbbcbe-b74e-4b45-8b6a-e0e70d1dd747",
   "metadata": {},
   "source": [
    "# Step 2: Converting dataset objects to pytorch dataset objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20efd75-09c1-4827-82df-691a2f2254c2",
   "metadata": {},
   "source": [
    "## a) Loading the normalization mean/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f2d1a6-f03b-44c3-8adc-718cb972021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasci_tools import pandas_utils as pu\n",
    "df_norm_filepath = \"./models/neuron_feature_normalization.csv\"\n",
    "df_norm = pu.csv_to_df(df_norm_filepath)\n",
    "df_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e4a080-96a2-4859-b6b7-4d439569c6fa",
   "metadata": {},
   "source": [
    "## b) Creating the class mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707fb694-ad1f-4e59-b061-76bc4a08361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_config = OutputClassConfig(gnnu.microns_cell_type_map)\n",
    "class_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4865de-9845-454e-afe6-6b2b715ebcae",
   "metadata": {},
   "source": [
    "## c) Creating Pytorch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3bf476-76e2-45e5-9577-e22dba87044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurd.gnn_cell_typing_utils import NeuronDataset\n",
    "\n",
    "torch_data_obj_neuron = NeuronDataset(\n",
    "    graph_data_list_neuron,\n",
    "    normalization_df=df_norm,\n",
    "    class_config = class_config,\n",
    ")\n",
    "# the pytorch geometric dataset \n",
    "torch_data_obj_neuron.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221e442-9739-488c-9b73-b2be6c854aba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch_data_obj_limb = NeuronDataset(\n",
    "    graph_data_list_limbs,\n",
    "    normalization_df=df_norm,\n",
    "    class_config = class_config,\n",
    ")\n",
    "\n",
    "# the pytorch geometric dataset \n",
    "torch_data_obj_limb.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b55afc7-faed-4eea-9940-957a615aebc4",
   "metadata": {},
   "source": [
    "## d) Creating a dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8fadb-dcbd-4be1-9f00-607b92d3acb0",
   "metadata": {},
   "source": [
    "# Step 3: Loading the Model and running inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aebd75-c82a-4bd5-be7a-0d21c139f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed130255-deb0-4928-8cc3-c9534c7a3058",
   "metadata": {},
   "source": [
    "## Option 1: Neuron Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff12786-4ed2-44a0-9cb2-3ee6c352728f",
   "metadata": {},
   "source": [
    "### a) Instantiating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85a7149-9817-4870-9524-521611c606d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture_kwargs = architecture_kwargs = dict(\n",
    "    num_node_features = torch_data_obj_neuron.dataset[0].x.shape[1],\n",
    "    num_classes = class_config.num_classes,\n",
    "    activation_function = \"relu\",\n",
    "    global_pool_type=\"mean_weighted\",\n",
    "    global_pool_weight = \"node_weight\",\n",
    "    \n",
    "    n_hidden_channels=128,\n",
    "    n_layers = 2,\n",
    "    \n",
    "    #batch norm specifics\n",
    "    use_bn = True,\n",
    "    track_running_stats=True,\n",
    ")\n",
    "\n",
    "model = gnnu.NeuronGCN(**architecture_kwargs)\n",
    "model_weights = \"./models/neuron_gnn_2_layer\"\n",
    "\n",
    "neuron_input_obj = gnnu.GnnInput(\n",
    "    dataset_obj = torch_data_obj_neuron,\n",
    "    model = model,\n",
    "    model_weights_filepath = model_weights,\n",
    "    class_config = class_config,\n",
    ")\n",
    "\n",
    "neuron_input_obj.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d45d513-0989-4a26-b211-4fb0f9fb7320",
   "metadata": {},
   "source": [
    "### b) Inference Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ee2e9-3a35-475a-b61f-11f7ed72bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_obj = gnnu.InferenceRunner(neuron_input_obj)\n",
    "inf_obj.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99979239-b6c4-4780-b851-653059c5deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_obj.prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b048d48c-825d-4dcc-8319-33c61ad0f10e",
   "metadata": {},
   "source": [
    "## Option 2: Limb Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe6561-dc3a-4e1d-9e2a-4e5c870b5b61",
   "metadata": {},
   "source": [
    "### a) Instantiating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c0f4a7-ff94-4c5f-bba9-5a7006392e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_kwargs_limb = dict(\n",
    "    num_node_features = torch_data_obj_limb.dataset[0].x.shape[1],\n",
    "    num_classes = class_config.num_classes,\n",
    "    activation_function = \"relu\",\n",
    "    global_pool_type=\"mean_weighted\",\n",
    "    global_pool_weight = \"node_weight\",\n",
    "    \n",
    "    n_hidden_channels=128,\n",
    "    n_layers = 2,\n",
    "    \n",
    "    #batch norm specifics\n",
    "    use_bn = True,\n",
    "    track_running_stats=True,\n",
    ")\n",
    "\n",
    "model_limb = gnnu.LimbGCN(**architecture_kwargs_limb)\n",
    "model_weights_limb = \"./models/limb_gnn_2_layer\"\n",
    "\n",
    "limb_input_obj = gnnu.GnnInput(\n",
    "    dataset_obj = torch_data_obj_limb,\n",
    "    model = model_limb,\n",
    "    model_weights_filepath = model_weights_limb,\n",
    "    class_config = class_config,\n",
    ")\n",
    "\n",
    "limb_input_obj.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767cf254-c5ef-49fb-a506-33e339c5bd22",
   "metadata": {},
   "source": [
    "### b) Inference Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8699101d-70fb-474d-a619-db64d7564e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_obj_limb = gnnu.InferenceRunner(limb_input_obj)\n",
    "inf_obj_limb.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df3ce43-a639-418b-a8e4-eaafbdec82c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: if neuron objects were not run with \n",
    "spine volume computed using a convex hull then some limbs\n",
    "may not be as accurate\n",
    "\"\"\"\n",
    "\n",
    "inf_obj_limb.prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851aa2e1-3550-4e92-8204-9e5fe7ac7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: the limb based model is lower performing becuase the \n",
    "newer neuron graph objects were generating using mesh volume \n",
    "computed using a higher fidelity method of mesh repair\n",
    "whereas the limb model was based on data where the mesh\n",
    "volume was computed using a convex hull. This fact along\n",
    "with the usage of max pooling in the limb based model\n",
    "is what makes this feature more likely to degrade the \n",
    "classification accuracy. To avoid this you could train \n",
    "your own limb based model or when computing the neuron\n",
    "object set the  mesh volume function to use a convex\n",
    "hull as mentioned earlier\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
